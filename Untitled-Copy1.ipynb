{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimention of train:\n",
      "(42001, 785)\n",
      "dimention of test:\n",
      "(28001, 784)\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "train = genfromtxt('/home/wwhlazio/Minst/data/train.csv',delimiter=',')\n",
    "test = genfromtxt('/home/wwhlazio/Minst/data/test.csv',delimiter=',')\n",
    "print(\"dimention of train:\")\n",
    "print(np.shape(train))\n",
    "print(\"dimention of test:\")\n",
    "print(np.shape(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784)\n",
      "(42000,)\n",
      "(42000,)\n",
      "(33600, 1, 28, 28)\n",
      "(33600,)\n",
      "(8400, 1, 28, 28)\n",
      "(8400,)\n",
      "(28000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#split train into train and validation\n",
    "#normalize the x\n",
    "random.seed(123)\n",
    "train_x=train[1:,1:]/255\n",
    "train_y=train[1:,0]\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "num_val=int(train_x.shape[0]*0.2)\n",
    "tmp=np.arange(train_x.shape[0])\n",
    "np.random.shuffle(tmp)\n",
    "print(tmp.shape)\n",
    "mask_val=tmp[0:num_val]\n",
    "mask_train=tmp[num_val:train_x.shape[0]]\n",
    "x_val=train_x[mask_val,:]\n",
    "y_val=train_y[mask_val]\n",
    "x_train=train_x[mask_train]\n",
    "y_train=train_y[mask_train]\n",
    "x_test=test[1:,:]/255\n",
    "\n",
    "x_test=x_test-np.mean(x_test,1,keepdims=True)\n",
    "x_test/=np.std(x_test,1,keepdims=True)\n",
    "x_train=x_train-np.mean(x_train,1,keepdims=True)\n",
    "x_train/=np.std(x_train,1,keepdims=True)\n",
    "x_val=x_val-np.mean(x_val,1,keepdims=True)\n",
    "x_val/=np.std(x_val,1,keepdims=True)\n",
    "x_test=np.reshape(x_test,(x_test.shape[0],1,28,28))\n",
    "x_train=np.reshape(x_train,(x_train.shape[0],1,28,28))\n",
    "x_val=np.reshape(x_val,(x_val.shape[0],1,28,28))\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a learning of cov --> ReLu --> pool --> cov --> ReLu --> pool --> cov --> ReLu --> pool --> fc --> norm --> dropout --> fc --> softmax \n",
    "#784 = 28*28\n",
    "from cs231n.layers import *\n",
    "from cs231n.fast_layers import *\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "#import fast_layers\n",
    "from cs231n.im2col_cython import col2im_cython, im2col_cython\n",
    "from cs231n.im2col_cython import col2im_6d_cython\n",
    "from cs231n.im2col_cython import *\n",
    "from cs231n import im2col\n",
    "from cs231n.im2col import *\n",
    "\n",
    "class minst_deep_new(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self,input_dim=(1,28,28),weight_scale=1e-3,stride=1,pad=1,filter_size=[3,3,2],num_filter=[64,64,64],pool_size=2,\n",
    "                 pool_stride=2,dropout_p=0.25,dropout_seed=123,reg=1e-4,num_class=10,hidden_dim=64):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.weight_scale=weight_scale\n",
    "        self.conv_params={}\n",
    "        self.conv_params['stride']=stride\n",
    "        self.conv_params['pad']=pad\n",
    "        self.filter_size=filter_size\n",
    "        self.num_filter=num_filter\n",
    "        self.pool_param = {'pool_height': pool_size, 'pool_width': pool_size, 'stride': pool_stride}\n",
    "        self.dropout_param = {'mode':'train','p': dropout_p, 'seed': dropout_seed}\n",
    "        self.num_class=num_class\n",
    "        self.reg=reg\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.bn_param={}\n",
    "        C, H, W=input_dim\n",
    "\n",
    "        self.params['W1']=weight_scale*np.random.randn(self.num_filter[0],1,self.filter_size[0],self.filter_size[0])\n",
    "        self.params['b1']=np.zeros((num_filter[0],1))\n",
    "        self.params['W2']=weight_scale*np.random.randn(self.num_filter[1],num_filter[0],self.filter_size[1],self.filter_size[1])\n",
    "        self.params['b2']=np.zeros((num_filter[1],1))\n",
    "        self.params['W22']=weight_scale*np.random.randn(self.num_filter[2],num_filter[1],self.filter_size[2],self.filter_size[2])\n",
    "        self.params['b22']=np.zeros((num_filter[2],1))\n",
    "        \n",
    "        H_prime=int(1+(H+2*self.conv_params['pad']-self.filter_size[0])/self.conv_params['stride'])\n",
    "        W_prime=int(1+(W+2*self.conv_params['pad']-self.filter_size[0])/self.conv_params['stride'])\n",
    "        \n",
    "        H_prime2=int((H_prime-self.pool_param['pool_height'])/self.pool_param['stride']+1)\n",
    "        W_prime2=int((W_prime-self.pool_param['pool_width'])/self.pool_param['stride']+1)\n",
    "        \n",
    "        print(H_prime2)\n",
    "        print(W_prime2)\n",
    "        \n",
    "        H_prime3=int(1+(H_prime2+2*self.conv_params['pad']-self.filter_size[0])/self.conv_params['stride'])\n",
    "        W_prime3=int(1+(W_prime2+2*self.conv_params['pad']-self.filter_size[0])/self.conv_params['stride'])\n",
    "        \n",
    "        H_prime4=int((H_prime3-self.pool_param['pool_height'])/self.pool_param['stride']+1)\n",
    "        W_prime4=int((W_prime3-self.pool_param['pool_width'])/self.pool_param['stride']+1)\n",
    "        \n",
    "        print('H4:%d' % H_prime4)\n",
    "        print('W4:%d' % W_prime4)\n",
    "        print('filter_size1:%d' % filter_size[1])\n",
    "        \n",
    "        H_prime5=int(1+(H_prime4+2*self.conv_params['pad']-self.filter_size[2])/self.conv_params['stride'])\n",
    "        W_prime5=int(1+(W_prime4+2*self.conv_params['pad']-self.filter_size[2])/self.conv_params['stride'])\n",
    "        \n",
    "        H_prime6=int((H_prime5-self.pool_param['pool_height'])/self.pool_param['stride']+1)\n",
    "        W_prime6=int((W_prime5-self.pool_param['pool_width'])/self.pool_param['stride']+1)\n",
    "        \n",
    "        print('H6:%d' % H_prime6)\n",
    "        print('W6:%d' % W_prime6)\n",
    "        print('filter_size1:%d' % filter_size[2])\n",
    "        \n",
    "        self.params['W3']=weight_scale*np.random.randn(self.num_filter[1]*H_prime6*W_prime6,self.hidden_dim)\n",
    "        self.params['b3']=np.zeros((hidden_dim,1))\n",
    "        self.H_prime6=H_prime6\n",
    "        self.W_prime6=W_prime6\n",
    "        self.params['gamma1']=np.ones(self.hidden_dim)\n",
    "        self.params['beta1']=np.zeros(self.hidden_dim)\n",
    "        self.params['W4']=weight_scale*np.random.randn(self.hidden_dim,self.num_class)\n",
    "        self.params['b4']=np.zeros((self.num_class,1))\n",
    "        self.bn_param['mode']='train'\n",
    "        \n",
    "        \n",
    "        print(\"initial parametrs shape:\")\n",
    "        print(np.shape(self.params['W1']))\n",
    "        print(np.shape(self.params['b1']))\n",
    "        print(np.shape(self.params['W2']))\n",
    "        print(np.shape(self.params['b2']))\n",
    "        print(np.shape(self.params['W22']))\n",
    "        print(np.shape(self.params['b22']))\n",
    "        print(np.shape(self.params['W3']))\n",
    "        print(np.shape(self.params['b3']))\n",
    "        print(np.shape(self.params['W4']))\n",
    "        print(np.shape(self.params['b4']))\n",
    "        print(np.shape(self.params['gamma1']))\n",
    "        print(np.shape(self.params['beta1']))\n",
    "        \n",
    "    def loss(self,x,y=None):\n",
    "        \n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        W22, b22 = self.params['W22'], self.params['b22']\n",
    "        W3, b3 = self.params['W3'], self.params['b3']\n",
    "        W4, b4 = self.params['W4'], self.params['b4']\n",
    "        gamma1,beta1=self.params['gamma1'],self.params['beta1']\n",
    "        \n",
    "        #mode = 'train'\n",
    "        #mode = 'test' if y is None else 'train'\n",
    "        #print(\"mode:\")\n",
    "        #print(mode)\n",
    "        #self.bn_param['mode'] = mode\n",
    "        #self.dropout_param['mode'] = mode\n",
    "        \n",
    "        num_train=x.shape[0]\n",
    "        \n",
    "        scores = None\n",
    "        #build a learning of cov --> pool --> cov --> pool --> fc --> dropout --> fc --> softmax\n",
    "        #print(x.shape)\n",
    "        #print(W1.shape)\n",
    "        #print(\"b1.shape:\")\n",
    "        #print(b1.shape)\n",
    "        a, conv_cache1=conv_forward_strides(x,W1,b1,self.conv_params)\n",
    "        aa, relu_cache1=relu_forward(a)\n",
    "        #print('aa.shape:')\n",
    "        #print(aa.shape)\n",
    "        b, pool_cache1=max_pool_forward_fast(aa,self.pool_param)\n",
    "        #print(\"b.shape:\")\n",
    "        #print(np.shape(b))\n",
    "        #print(np.shape(W2))\n",
    "        #print(np.shape(b2))\n",
    "        c, conv_cache2=conv_forward_strides(b,W2,b2,self.conv_params)\n",
    "        cc, relu_cache2=relu_forward(c)\n",
    "        d, pool_cache2=max_pool_forward_fast(cc,self.pool_param)\n",
    "        #print(np.shape(c))\n",
    "        #print(np.shape(cc))\n",
    "        #print(\"d.shape:\")\n",
    "        #print(np.shape(d))\n",
    "        c1, conv_cache3=conv_forward_strides(d,W22,b22,self.conv_params)\n",
    "        #print(c1.shape)\n",
    "        cc1, relu_cache3=relu_forward(c1)\n",
    "        #print(cc1.shape)\n",
    "        d1, pool_cache3=max_pool_forward_fast(cc1,self.pool_param)\n",
    "        \n",
    "        dd,aff_cache1=affine_forward(d1,W3,b3)\n",
    "        ddd,bn_cache1=batchnorm_forward(dd,gamma1,beta1,self.bn_param)\n",
    "        e, dp_cache = dropout_forward(ddd,self.dropout_param)\n",
    "        #print('eshape:')\n",
    "        #print(e.shape)\n",
    "        #print(ddd.shape)\n",
    "        #print('dd.shape:')\n",
    "        #print(dd.shape)\n",
    "        scores, aff_cache2=affine_forward(e,W4,b4)\n",
    "        \n",
    "        if y is None:\n",
    "            return scores\n",
    "        \n",
    "        loss,grads = 0,{}\n",
    "        \n",
    "        \n",
    "        scores=scores-np.max(scores,axis=1,keepdims=True)\n",
    "        escores=np.exp(scores)\n",
    "        secores=np.sum(escores,1,keepdims=True)\n",
    "        escoresx=escores/secores\n",
    "        scores=escoresx\n",
    "        esc=escoresx\n",
    "        #print(y)\n",
    "        #print(dtype(y))\n",
    "        y=y.astype(int)\n",
    "        escoresx=-np.log(escoresx[range(num_train),y]+1e-8)\n",
    "        loss=np.sum(escoresx)/num_train\n",
    "        loss+=self.reg/2*(np.sum(W1*W1)+np.sum(W2*W2)+np.sum(W3*W3))\n",
    "        \n",
    "        esc[range(num_train),y]-=1\n",
    "        esc/=num_train\n",
    "        dout=esc\n",
    "        #dout=np.reshape(dout,(dout.shape[0],-1))\n",
    "        #print(\"doutshape:\")\n",
    "        #print(np.shape(e))\n",
    "        e=np.reshape(e,(e.shape[0],-1))\n",
    "        \n",
    "        grads['W4']=e.T.dot(dout)+self.reg*W4\n",
    "        grads['b4']=np.sum(dout,axis=0,keepdims=True)\n",
    "        #print(\"b3shape:\")\n",
    "        #print(np.shape(grads['b3']))\n",
    "        #print(\"doutshape:\")\n",
    "        #print(np.shape(dout))\n",
    "        grads['b4'].shape=(grads['b4'].shape[1],1)\n",
    "        grads_affine2=np.dot(dout,W4.T)\n",
    "        #print('affine2dim:')\n",
    "        #print(np.shape(grads_affine2))\n",
    "        #grads_affine2=np.reshape(grads_affine2,(grads_affine2.shape[0],self.num_filter[1],self.H_prime4,self.W_prime4))\n",
    "        \n",
    "        grads_dp=dropout_backward(grads_affine2,dp_cache)\n",
    "        grads_norm,grads['gamma1'],grads['beta1']=batchnorm_backward(grads_dp,bn_cache1)\n",
    "        #print('grads_norm.shape:')\n",
    "        #print(grads_norm.shape)\n",
    "        grads_affine1,grads['W3'],grads['b3']=affine_backward(grads_norm,aff_cache1)\n",
    "        #print('grads_affine1.shape:')\n",
    "        #print(np.shape(grads_affine1))\n",
    "        grads_affine1=np.reshape(grads_affine1,(grads_affine1.shape[0],self.num_filter[1],self.H_prime6,self.W_prime6))\n",
    "        grads_max3=max_pool_backward_fast(grads_affine1,pool_cache3)\n",
    "        grads_relu3=relu_backward(grads_max3, relu_cache3)\n",
    "        grads_cov3, grads['W22'], grads['b22']=conv_backward_strides(grads_relu3,conv_cache3)\n",
    "        grads['b22'].shape=(grads['b22'].shape[0],1)\n",
    "        grads_max2=max_pool_backward_fast(grads_cov3,pool_cache2)\n",
    "        grads_relu2=relu_backward(grads_max2, relu_cache2)\n",
    "        grads_cov2, grads['W2'], grads['b2']=conv_backward_strides(grads_relu2,conv_cache2)\n",
    "        grads['b2'].shape=(grads['b2'].shape[0],1)\n",
    "        grads_max1=max_pool_backward_fast(grads_cov2,pool_cache1)\n",
    "        grads_relu1=relu_backward(grads_max1, relu_cache1)\n",
    "        grads_cov1, grads['W1'], grads['b1']=conv_backward_strides(grads_relu1,conv_cache1)\n",
    "        grads['b1'].shape=(grads['b1'].shape[0],1)\n",
    "        #print(\"gradb1:\")\n",
    "        #print(np.shape(grads['b1']))\n",
    "                                  \n",
    "        return loss, grads\n",
    "\n",
    "                                      \n",
    "                            \n",
    "                                  \n",
    "                    \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "14\n",
      "H4:7\n",
      "W4:7\n",
      "filter_size1:3\n",
      "H6:4\n",
      "W6:4\n",
      "filter_size1:2\n",
      "initial parametrs shape:\n",
      "(64, 1, 3, 3)\n",
      "(64, 1)\n",
      "(64, 64, 3, 3)\n",
      "(64, 1)\n",
      "(64, 64, 2, 2)\n",
      "(64, 1)\n",
      "(1024, 64)\n",
      "(64, 1)\n",
      "(64, 10)\n",
      "(10, 1)\n",
      "(64,)\n",
      "(64,)\n",
      "(Iteration 1 / 40) loss: 2.303351 acc: 0.100000\n",
      "(Epoch 0 / 20) train acc: 0.600000; val_acc: 0.432738\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 2 / 40) loss: 2.295233 acc: 0.560000\n",
      "(Epoch 1 / 20) train acc: 0.580000; val_acc: 0.471310\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 3 / 40) loss: 2.265336 acc: 0.760000\n",
      "(Iteration 4 / 40) loss: 2.217426 acc: 0.520000\n",
      "(Epoch 2 / 20) train acc: 0.590000; val_acc: 0.469881\n",
      "(Iteration 5 / 40) loss: 2.095860 acc: 0.740000\n",
      "(Iteration 6 / 40) loss: 2.051168 acc: 0.580000\n",
      "(Epoch 3 / 20) train acc: 0.630000; val_acc: 0.480119\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 7 / 40) loss: 1.966362 acc: 0.780000\n",
      "(Iteration 8 / 40) loss: 1.952424 acc: 0.680000\n",
      "(Epoch 4 / 20) train acc: 0.700000; val_acc: 0.511310\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 9 / 40) loss: 1.884083 acc: 0.860000\n",
      "(Iteration 10 / 40) loss: 1.869755 acc: 0.800000\n",
      "(Epoch 5 / 20) train acc: 0.750000; val_acc: 0.545833\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 11 / 40) loss: 1.806939 acc: 0.900000\n",
      "(Iteration 12 / 40) loss: 1.792000 acc: 0.840000\n",
      "(Epoch 6 / 20) train acc: 0.760000; val_acc: 0.576071\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 13 / 40) loss: 1.730279 acc: 0.900000\n",
      "(Iteration 14 / 40) loss: 1.715317 acc: 0.840000\n",
      "(Epoch 7 / 20) train acc: 0.770000; val_acc: 0.592976\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 15 / 40) loss: 1.651726 acc: 0.920000\n",
      "(Iteration 16 / 40) loss: 1.639188 acc: 0.860000\n",
      "(Epoch 8 / 20) train acc: 0.770000; val_acc: 0.605238\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 17 / 40) loss: 1.572582 acc: 0.960000\n",
      "(Iteration 18 / 40) loss: 1.563446 acc: 0.900000\n",
      "(Epoch 9 / 20) train acc: 0.780000; val_acc: 0.617262\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 19 / 40) loss: 1.493544 acc: 0.980000\n",
      "(Iteration 20 / 40) loss: 1.487274 acc: 0.900000\n",
      "(Epoch 10 / 20) train acc: 0.810000; val_acc: 0.634405\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 21 / 40) loss: 1.413407 acc: 1.000000\n",
      "(Iteration 22 / 40) loss: 1.409053 acc: 0.940000\n",
      "(Epoch 11 / 20) train acc: 0.830000; val_acc: 0.652143\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 23 / 40) loss: 1.331980 acc: 1.000000\n",
      "(Iteration 24 / 40) loss: 1.330783 acc: 0.940000\n",
      "(Epoch 12 / 20) train acc: 0.840000; val_acc: 0.674643\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 25 / 40) loss: 1.250754 acc: 1.000000\n",
      "(Iteration 26 / 40) loss: 1.255034 acc: 0.940000\n",
      "(Epoch 13 / 20) train acc: 0.860000; val_acc: 0.693452\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 27 / 40) loss: 1.171136 acc: 1.000000\n",
      "(Iteration 28 / 40) loss: 1.180381 acc: 0.980000\n",
      "(Epoch 14 / 20) train acc: 0.880000; val_acc: 0.709762\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 29 / 40) loss: 1.092836 acc: 1.000000\n",
      "(Iteration 30 / 40) loss: 1.105643 acc: 1.000000\n",
      "(Epoch 15 / 20) train acc: 0.880000; val_acc: 0.725238\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 31 / 40) loss: 1.016843 acc: 1.000000\n",
      "(Iteration 32 / 40) loss: 1.031935 acc: 1.000000\n",
      "(Epoch 16 / 20) train acc: 0.910000; val_acc: 0.737500\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 33 / 40) loss: 0.943323 acc: 1.000000\n",
      "(Iteration 34 / 40) loss: 0.961084 acc: 1.000000\n",
      "(Epoch 17 / 20) train acc: 0.910000; val_acc: 0.744643\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 35 / 40) loss: 0.873713 acc: 1.000000\n",
      "(Iteration 36 / 40) loss: 0.893722 acc: 1.000000\n",
      "(Epoch 18 / 20) train acc: 0.920000; val_acc: 0.746667\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 37 / 40) loss: 0.807952 acc: 1.000000\n",
      "(Iteration 38 / 40) loss: 0.829485 acc: 1.000000\n",
      "(Epoch 19 / 20) train acc: 0.920000; val_acc: 0.751190\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 39 / 40) loss: 0.746533 acc: 1.000000\n",
      "(Iteration 40 / 40) loss: 0.768050 acc: 1.000000\n",
      "(Epoch 20 / 20) train acc: 0.920000; val_acc: 0.752976\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "#overfit small size data\n",
    "from __future__ import print_function\n",
    "from cs231n.solver import *\n",
    "from cs231n import optim\n",
    "from cs231n.optim import *\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "#import fast_layers\n",
    "from cs231n.im2col_cython import col2im_cython, im2col_cython\n",
    "from cs231n.im2col_cython import col2im_6d_cython\n",
    "from cs231n.im2col_cython import *\n",
    "from cs231n import im2col\n",
    "from cs231n.im2col import *\n",
    "\n",
    "np.random.seed(231)\n",
    "\n",
    "num_train = 100\n",
    "small_data = {\n",
    "  'X_train': x_train[:num_train],\n",
    "  'y_train': y_train[:num_train],\n",
    "  'X_val': x_val,\n",
    "  'y_val': y_val,\n",
    "}\n",
    "\n",
    "model = minst_deep_new(weight_scale=1e-2)\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=20, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },verbose=True, print_every=1)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from future import standard_library\n",
    "standard_library.install_aliases()\n",
    "from builtins import range\n",
    "from builtins import object\n",
    "import os\n",
    "import pickle as pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from cs231n import optim\n",
    "hasattr(optim,\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH4ZhiYXJtCUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o')\n",
    "plt.plot(solver.val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "14\n",
      "H4:7\n",
      "W4:7\n",
      "filter_size1:3\n",
      "H6:4\n",
      "W6:4\n",
      "filter_size1:2\n",
      "initial parametrs shape:\n",
      "(64, 1, 3, 3)\n",
      "(64, 1)\n",
      "(64, 64, 3, 3)\n",
      "(64, 1)\n",
      "(64, 64, 2, 2)\n",
      "(64, 1)\n",
      "(1024, 64)\n",
      "(64, 1)\n",
      "(64, 10)\n",
      "(10, 1)\n",
      "(64,)\n",
      "(64,)\n",
      "(Iteration 1 / 3360) loss: 2.303834 acc: 0.020000\n",
      "(Epoch 0 / 5) train acc: 0.339000; val_acc: 0.358929\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 21 / 3360) loss: 1.336273 acc: 1.000000\n",
      "(Iteration 41 / 3360) loss: 0.576323 acc: 1.000000\n",
      "(Iteration 61 / 3360) loss: 0.230144 acc: 1.000000\n",
      "(Iteration 81 / 3360) loss: 0.115025 acc: 1.000000\n",
      "(Iteration 101 / 3360) loss: 0.071192 acc: 1.000000\n",
      "(Iteration 121 / 3360) loss: 0.050042 acc: 1.000000\n",
      "(Iteration 141 / 3360) loss: 0.037865 acc: 1.000000\n",
      "(Iteration 161 / 3360) loss: 0.030036 acc: 1.000000\n",
      "(Iteration 181 / 3360) loss: 0.024634 acc: 1.000000\n",
      "(Iteration 201 / 3360) loss: 0.020719 acc: 1.000000\n",
      "(Iteration 221 / 3360) loss: 0.017777 acc: 1.000000\n",
      "(Iteration 241 / 3360) loss: 0.015501 acc: 1.000000\n",
      "(Iteration 261 / 3360) loss: 0.013701 acc: 1.000000\n",
      "(Iteration 281 / 3360) loss: 0.012249 acc: 1.000000\n",
      "(Iteration 301 / 3360) loss: 0.011058 acc: 1.000000\n",
      "(Iteration 321 / 3360) loss: 0.010069 acc: 1.000000\n",
      "(Iteration 341 / 3360) loss: 0.009236 acc: 1.000000\n",
      "(Iteration 361 / 3360) loss: 0.008529 acc: 1.000000\n",
      "(Iteration 381 / 3360) loss: 0.007922 acc: 1.000000\n",
      "(Iteration 401 / 3360) loss: 0.007398 acc: 1.000000\n",
      "(Iteration 421 / 3360) loss: 0.006940 acc: 1.000000\n",
      "(Iteration 441 / 3360) loss: 0.006539 acc: 1.000000\n",
      "(Iteration 461 / 3360) loss: 0.006186 acc: 1.000000\n",
      "(Iteration 481 / 3360) loss: 0.005872 acc: 1.000000\n",
      "(Iteration 501 / 3360) loss: 0.005592 acc: 1.000000\n",
      "(Iteration 521 / 3360) loss: 0.005341 acc: 1.000000\n",
      "(Iteration 541 / 3360) loss: 0.005116 acc: 1.000000\n",
      "(Iteration 561 / 3360) loss: 0.004912 acc: 1.000000\n",
      "(Iteration 581 / 3360) loss: 0.004728 acc: 1.000000\n",
      "(Iteration 601 / 3360) loss: 0.004560 acc: 1.000000\n",
      "(Iteration 621 / 3360) loss: 0.004407 acc: 1.000000\n",
      "(Iteration 641 / 3360) loss: 0.004267 acc: 1.000000\n",
      "(Iteration 661 / 3360) loss: 0.004139 acc: 1.000000\n",
      "(Epoch 1 / 5) train acc: 0.729000; val_acc: 0.699643\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 681 / 3360) loss: 0.022691 acc: 1.000000\n",
      "(Iteration 701 / 3360) loss: 0.008888 acc: 1.000000\n",
      "(Iteration 721 / 3360) loss: 0.006920 acc: 1.000000\n",
      "(Iteration 741 / 3360) loss: 0.006295 acc: 1.000000\n",
      "(Iteration 761 / 3360) loss: 0.005955 acc: 1.000000\n",
      "(Iteration 781 / 3360) loss: 0.005724 acc: 1.000000\n",
      "(Iteration 801 / 3360) loss: 0.005551 acc: 1.000000\n",
      "(Iteration 821 / 3360) loss: 0.005413 acc: 1.000000\n",
      "(Iteration 841 / 3360) loss: 0.005299 acc: 1.000000\n",
      "(Iteration 861 / 3360) loss: 0.005202 acc: 1.000000\n",
      "(Iteration 881 / 3360) loss: 0.005119 acc: 1.000000\n",
      "(Iteration 901 / 3360) loss: 0.005046 acc: 1.000000\n",
      "(Iteration 921 / 3360) loss: 0.004982 acc: 1.000000\n",
      "(Iteration 941 / 3360) loss: 0.004924 acc: 1.000000\n",
      "(Iteration 961 / 3360) loss: 0.004872 acc: 1.000000\n",
      "(Iteration 981 / 3360) loss: 0.004825 acc: 1.000000\n",
      "(Iteration 1001 / 3360) loss: 0.004782 acc: 1.000000\n",
      "(Iteration 1021 / 3360) loss: 0.004743 acc: 1.000000\n",
      "(Iteration 1041 / 3360) loss: 0.004706 acc: 1.000000\n",
      "(Iteration 1061 / 3360) loss: 0.004673 acc: 1.000000\n",
      "(Iteration 1081 / 3360) loss: 0.004641 acc: 1.000000\n",
      "(Iteration 1101 / 3360) loss: 0.004612 acc: 1.000000\n",
      "(Iteration 1121 / 3360) loss: 0.004585 acc: 1.000000\n",
      "(Iteration 1141 / 3360) loss: 0.004560 acc: 1.000000\n",
      "(Iteration 1161 / 3360) loss: 0.004536 acc: 1.000000\n",
      "(Iteration 1181 / 3360) loss: 0.004513 acc: 1.000000\n",
      "(Iteration 1201 / 3360) loss: 0.004492 acc: 1.000000\n",
      "(Iteration 1221 / 3360) loss: 0.004472 acc: 1.000000\n",
      "(Iteration 1241 / 3360) loss: 0.004453 acc: 1.000000\n",
      "(Iteration 1261 / 3360) loss: 0.004435 acc: 1.000000\n",
      "(Iteration 1281 / 3360) loss: 0.004418 acc: 1.000000\n",
      "(Iteration 1301 / 3360) loss: 0.004402 acc: 1.000000\n",
      "(Iteration 1321 / 3360) loss: 0.004386 acc: 1.000000\n",
      "(Iteration 1341 / 3360) loss: 0.004371 acc: 1.000000\n",
      "(Epoch 2 / 5) train acc: 0.756000; val_acc: 0.724167\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 1361 / 3360) loss: 0.006990 acc: 1.000000\n",
      "(Iteration 1381 / 3360) loss: 0.005356 acc: 1.000000\n",
      "(Iteration 1401 / 3360) loss: 0.005033 acc: 1.000000\n",
      "(Iteration 1421 / 3360) loss: 0.004912 acc: 1.000000\n",
      "(Iteration 1441 / 3360) loss: 0.004842 acc: 1.000000\n",
      "(Iteration 1461 / 3360) loss: 0.004793 acc: 1.000000\n",
      "(Iteration 1481 / 3360) loss: 0.004755 acc: 1.000000\n",
      "(Iteration 1501 / 3360) loss: 0.004725 acc: 1.000000\n",
      "(Iteration 1521 / 3360) loss: 0.004699 acc: 1.000000\n",
      "(Iteration 1541 / 3360) loss: 0.004678 acc: 1.000000\n",
      "(Iteration 1561 / 3360) loss: 0.004659 acc: 1.000000\n",
      "(Iteration 1581 / 3360) loss: 0.004643 acc: 1.000000\n",
      "(Iteration 1601 / 3360) loss: 0.004628 acc: 1.000000\n",
      "(Iteration 1621 / 3360) loss: 0.004615 acc: 1.000000\n",
      "(Iteration 1641 / 3360) loss: 0.004603 acc: 1.000000\n",
      "(Iteration 1661 / 3360) loss: 0.004592 acc: 1.000000\n",
      "(Iteration 1681 / 3360) loss: 0.004582 acc: 1.000000\n",
      "(Iteration 1701 / 3360) loss: 0.004573 acc: 1.000000\n",
      "(Iteration 1721 / 3360) loss: 0.004564 acc: 1.000000\n",
      "(Iteration 1741 / 3360) loss: 0.004556 acc: 1.000000\n",
      "(Iteration 1761 / 3360) loss: 0.004548 acc: 1.000000\n",
      "(Iteration 1781 / 3360) loss: 0.004541 acc: 1.000000\n",
      "(Iteration 1801 / 3360) loss: 0.004535 acc: 1.000000\n",
      "(Iteration 1821 / 3360) loss: 0.004528 acc: 1.000000\n",
      "(Iteration 1841 / 3360) loss: 0.004522 acc: 1.000000\n",
      "(Iteration 1861 / 3360) loss: 0.004516 acc: 1.000000\n",
      "(Iteration 1881 / 3360) loss: 0.004511 acc: 1.000000\n",
      "(Iteration 1901 / 3360) loss: 0.004506 acc: 1.000000\n",
      "(Iteration 1921 / 3360) loss: 0.004501 acc: 1.000000\n",
      "(Iteration 1941 / 3360) loss: 0.004496 acc: 1.000000\n",
      "(Iteration 1961 / 3360) loss: 0.004491 acc: 1.000000\n",
      "(Iteration 1981 / 3360) loss: 0.004487 acc: 1.000000\n",
      "(Iteration 2001 / 3360) loss: 0.004483 acc: 1.000000\n",
      "(Epoch 3 / 5) train acc: 0.756000; val_acc: 0.739643\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n",
      "(Iteration 2021 / 3360) loss: 0.004539 acc: 1.000000\n",
      "(Iteration 2041 / 3360) loss: 0.004615 acc: 1.000000\n",
      "(Iteration 2061 / 3360) loss: 0.004538 acc: 1.000000\n",
      "(Iteration 2081 / 3360) loss: 0.004513 acc: 1.000000\n",
      "(Iteration 2101 / 3360) loss: 0.004501 acc: 1.000000\n",
      "(Iteration 2121 / 3360) loss: 0.004492 acc: 1.000000\n",
      "(Iteration 2141 / 3360) loss: 0.004486 acc: 1.000000\n",
      "(Iteration 2161 / 3360) loss: 0.004480 acc: 1.000000\n",
      "(Iteration 2181 / 3360) loss: 0.004476 acc: 1.000000\n",
      "(Iteration 2201 / 3360) loss: 0.004472 acc: 1.000000\n",
      "(Iteration 2221 / 3360) loss: 0.004468 acc: 1.000000\n",
      "(Iteration 2241 / 3360) loss: 0.004465 acc: 1.000000\n",
      "(Iteration 2261 / 3360) loss: 0.004462 acc: 1.000000\n",
      "(Iteration 2281 / 3360) loss: 0.004459 acc: 1.000000\n",
      "(Iteration 2301 / 3360) loss: 0.004457 acc: 1.000000\n",
      "(Iteration 2321 / 3360) loss: 0.004454 acc: 1.000000\n",
      "(Iteration 2341 / 3360) loss: 0.004452 acc: 1.000000\n",
      "(Iteration 2361 / 3360) loss: 0.004449 acc: 1.000000\n",
      "(Iteration 2381 / 3360) loss: 0.004447 acc: 1.000000\n",
      "(Iteration 2401 / 3360) loss: 0.004445 acc: 1.000000\n",
      "(Iteration 2421 / 3360) loss: 0.004443 acc: 1.000000\n",
      "(Iteration 2441 / 3360) loss: 0.004441 acc: 1.000000\n",
      "(Iteration 2461 / 3360) loss: 0.004439 acc: 1.000000\n",
      "(Iteration 2481 / 3360) loss: 0.004437 acc: 1.000000\n",
      "(Iteration 2501 / 3360) loss: 0.004436 acc: 1.000000\n",
      "(Iteration 2521 / 3360) loss: 0.004434 acc: 1.000000\n",
      "(Iteration 2541 / 3360) loss: 0.004432 acc: 1.000000\n",
      "(Iteration 2561 / 3360) loss: 0.004430 acc: 1.000000\n",
      "(Iteration 2581 / 3360) loss: 0.004429 acc: 1.000000\n",
      "(Iteration 2601 / 3360) loss: 0.004427 acc: 1.000000\n",
      "(Iteration 2621 / 3360) loss: 0.004426 acc: 1.000000\n",
      "(Iteration 2641 / 3360) loss: 0.004424 acc: 1.000000\n",
      "(Iteration 2661 / 3360) loss: 0.004423 acc: 1.000000\n",
      "(Iteration 2681 / 3360) loss: 0.004422 acc: 1.000000\n",
      "(Epoch 4 / 5) train acc: 0.765000; val_acc: 0.746429\n",
      "W1\n",
      "(64, 1, 3, 3)\n",
      "b1\n",
      "(64, 1)\n",
      "W2\n",
      "(64, 64, 3, 3)\n",
      "b2\n",
      "(64, 1)\n",
      "W22\n",
      "(64, 64, 2, 2)\n",
      "b22\n",
      "(64, 1)\n",
      "W3\n",
      "(1024, 64)\n",
      "b3\n",
      "(64, 1)\n",
      "gamma1\n",
      "(64,)\n",
      "beta1\n",
      "(64,)\n",
      "W4\n",
      "(64, 10)\n",
      "b4\n",
      "(10, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 2701 / 3360) loss: 0.004444 acc: 1.000000\n",
      "(Iteration 2721 / 3360) loss: 0.004436 acc: 1.000000\n",
      "(Iteration 2741 / 3360) loss: 0.004428 acc: 1.000000\n",
      "(Iteration 2761 / 3360) loss: 0.004424 acc: 1.000000\n",
      "(Iteration 2781 / 3360) loss: 0.004421 acc: 1.000000\n",
      "(Iteration 2801 / 3360) loss: 0.004419 acc: 1.000000\n",
      "(Iteration 2821 / 3360) loss: 0.004418 acc: 1.000000\n",
      "(Iteration 2841 / 3360) loss: 0.004416 acc: 1.000000\n",
      "(Iteration 2861 / 3360) loss: 0.004415 acc: 1.000000\n",
      "(Iteration 2881 / 3360) loss: 0.004414 acc: 1.000000\n",
      "(Iteration 2901 / 3360) loss: 0.004413 acc: 1.000000\n",
      "(Iteration 2921 / 3360) loss: 0.004412 acc: 1.000000\n",
      "(Iteration 2941 / 3360) loss: 0.004411 acc: 1.000000\n",
      "(Iteration 2961 / 3360) loss: 0.004410 acc: 1.000000\n",
      "(Iteration 2981 / 3360) loss: 0.004409 acc: 1.000000\n",
      "(Iteration 3001 / 3360) loss: 0.004408 acc: 1.000000\n",
      "(Iteration 3021 / 3360) loss: 0.004407 acc: 1.000000\n",
      "(Iteration 3041 / 3360) loss: 0.004406 acc: 1.000000\n",
      "(Iteration 3061 / 3360) loss: 0.004405 acc: 1.000000\n",
      "(Iteration 3081 / 3360) loss: 0.004404 acc: 1.000000\n",
      "(Iteration 3101 / 3360) loss: 0.004403 acc: 1.000000\n",
      "(Iteration 3121 / 3360) loss: 0.004403 acc: 1.000000\n",
      "(Iteration 3141 / 3360) loss: 0.004402 acc: 1.000000\n",
      "(Iteration 3161 / 3360) loss: 0.004401 acc: 1.000000\n",
      "(Iteration 3181 / 3360) loss: 0.004400 acc: 1.000000\n",
      "(Iteration 3201 / 3360) loss: 0.004400 acc: 1.000000\n",
      "(Iteration 3221 / 3360) loss: 0.004399 acc: 1.000000\n",
      "(Iteration 3241 / 3360) loss: 0.004398 acc: 1.000000\n",
      "(Iteration 3261 / 3360) loss: 0.004398 acc: 1.000000\n",
      "(Iteration 3281 / 3360) loss: 0.004397 acc: 1.000000\n",
      "(Iteration 3301 / 3360) loss: 0.004396 acc: 1.000000\n",
      "(Iteration 3321 / 3360) loss: 0.004396 acc: 1.000000\n",
      "(Iteration 3341 / 3360) loss: 0.004395 acc: 1.000000\n",
      "(Epoch 5 / 5) train acc: 0.765000; val_acc: 0.744286\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from cs231n.solver import *\n",
    "from cs231n import optim\n",
    "from cs231n.optim import *\n",
    "\n",
    "np.random.seed(231)\n",
    "model = minst_deep_new(weight_scale=1e-2)\n",
    "\n",
    "data = {\n",
    "  'X_train': x_train,\n",
    "  'y_train': y_train,\n",
    "  'X_val': x_val,\n",
    "  'y_val': y_val,\n",
    "}\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                num_epochs=5, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },verbose=True, print_every=20)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wd7PNTicBdHGgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fec9417aa20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o')\n",
    "plt.plot(solver.val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(231)\n",
    "#model = minst_deep_new(weight_scale=1e-2)\n",
    "y_test_pred = np.argmax(model.loss(x_test), axis=1)\n",
    "y_val_pred = np.argmax(model.loss(x_val), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == y_val).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 9 1 6 2 7 8 9 3]\n",
      "[ 3.  4.  9.  1.  6.  2.  7.  8.  9.  3.]\n"
     ]
    }
   ],
   "source": [
    "print(y_val_pred[0:10])\n",
    "print(y_val[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=np.array(range(28000))+1\n",
    "tmp.shape=(tmp.shape[0],1)\n",
    "print(tmp.shape)\n",
    "y_test_pred.shape=(y_test_pred.shape[0],1)\n",
    "print(y_test_pred.shape)\n",
    "res=np.concatenate((tmp,y_test_pred),axis=1)\n",
    "print(res.shape)\n",
    "print(res[0:5,])\n",
    "np.savetxt(\"/hpc/users/wangm08/Wenhui/drug/Minst/result/submit_test_new.csv\", np.rint(res), delimiter=\",\", fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
